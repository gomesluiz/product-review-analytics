{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "RANDOM_SEED = 19730115\n",
    "NUMBER_OF_WORDS = 50\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "#1591632\n",
    "logging.info(\"Required packages installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, stratify=False):\n",
    "    \"\"\"Get the data from csv file\n",
    "\n",
    "    Args:\n",
    "        path(str): the file complete path. \n",
    "\n",
    "    Returns:\n",
    "        dataframe: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    if stratify:\n",
    "        dataset = dataset.groupby('polarity', group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=0.4))\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/buscape_reviews_train_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/product-review-analytics/notebooks/04 - Feature Engineering.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the reviews datasets.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m reviews_train_dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m../data/processed/buscape_reviews_train_dataset.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m reviews_test_dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m../data/processed/buscape_reviews_test_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/workspaces/product-review-analytics/notebooks/04 - Feature Engineering.ipynb Cell 4\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, stratify)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(path, stratify\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"Get the data from csv file\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m        dataframe: A pandas dataframe.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m stratify:\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mpolarity\u001b[39m\u001b[39m'\u001b[39m, group_keys\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B5c5c77736c2e6c6f63616c686f73745c5562756e74755c686f6d655c676f6d65736c75697a5c576f726b73706163655c70726f647563742d7265766965772d616e616c7974696373/workspaces/product-review-analytics/notebooks/04%20-%20Feature%20Engineering.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/buscape_reviews_train_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Load the reviews datasets.\n",
    "reviews_train_dataset = load_dataset(\n",
    "    \"../data/processed/buscape_reviews_train_dataset.csv\", True)\n",
    "reviews_test_dataset = load_dataset(\n",
    "    \"../data/processed/buscape_reviews_test_dataset.csv\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Polarity Distribution in Train')\n",
    "reviews_train_dataset['polarity'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Polarity Distribution in Test')\n",
    "reviews_test_dataset['polarity'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS)\n",
    "reviews_train_cv = cv.fit_transform(\n",
    "    reviews_train_dataset['review_text_cleaned_no_stopwords'])\n",
    "reviews_train_dtm_cv = pd.DataFrame(\n",
    "    reviews_train_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "reviews_train_dtm_cv.index = reviews_train_dataset.index\n",
    "reviews_train_processed_cv = pd.concat([reviews_train_dataset[[\n",
    "                                       'original_index']], reviews_train_dtm_cv, reviews_train_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The counter vectorizer train matrix has {reviews_train_processed_cv.shape[0]} rows and {reviews_train_processed_cv.shape[1]} columns\")\n",
    "\n",
    "reviews_test_cv = cv.transform(\n",
    "    reviews_test_dataset['review_text_cleaned_no_stopwords'])\n",
    "reviews_test_dtm_cv = pd.DataFrame(\n",
    "    reviews_test_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "reviews_test_dtm_cv.index = reviews_test_dataset.index\n",
    "reviews_test_processed_cv = pd.concat([reviews_test_dataset[[\n",
    "                                      'original_index']], reviews_test_dtm_cv, reviews_test_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The counter vectorizer test matrix has {reviews_test_processed_cv.shape[0]} rows and {reviews_test_processed_cv.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_cv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_processed_cv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_cv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_train_dataset_cv_s{NUMBER_OF_WORDS}.pkl')\n",
    "reviews_test_processed_cv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_test_dataset_cv_s{NUMBER_OF_WORDS}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=50)\n",
    "reviews_train_tv = tv.fit_transform(reviews_train_dataset['review_text'])\n",
    "reviews_train_dtm_tv = pd.DataFrame(\n",
    "    reviews_train_tv.toarray(), columns=tv.get_feature_names_out())\n",
    "reviews_train_dtm_tv.index = reviews_train_dataset.index\n",
    "reviews_train_processed_tv = pd.concat([reviews_train_dataset[[\n",
    "                                       'original_index']], reviews_train_dtm_tv, reviews_train_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The tf-idf vectorizer train matrix has {reviews_train_processed_tv.shape[0]} rows and {reviews_train_processed_tv.shape[1]} columns\")\n",
    "\n",
    "reviews_test_tv = tv.transform(reviews_test_dataset['review_text'])\n",
    "reviews_test_dtm_tv = pd.DataFrame(\n",
    "    reviews_test_tv.toarray(), columns=tv.get_feature_names_out())\n",
    "reviews_test_dtm_tv.index = reviews_test_dataset.index\n",
    "reviews_test_processed_tv = pd.concat([reviews_test_dataset[[\n",
    "                                      'original_index']], reviews_test_dtm_tv, reviews_test_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The tf-idf vectorizer test matrix has {reviews_test_processed_tv.shape[0]} rows and {reviews_test_processed_tv.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_tv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_tv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_train_dataset_tv_s{NUMBER_OF_WORDS}.pkl')\n",
    "reviews_test_processed_tv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_test_dataset_tv_s{NUMBER_OF_WORDS}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logging.info(\"Load fast text embeddings.\")\n",
    "fasttext_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/fasttext_cbow_s50/cbow_s50.txt')\n",
    "fasttext_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/fasttext_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned glove embedding.\n",
    "logging.info(\"Load glove embeddings.\")\n",
    "glove_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/glove_s50/glove_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logging.info(\"Load wang2vec embeddings.\")\n",
    "wang2vec_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/wang2vec_cbow_s50/cbow_s50.txt')\n",
    "wang2vec_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/wang2vec_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned word2vec embedding.\n",
    "logging.info(\"Load word2vec embeddings.\")\n",
    "word2vec_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/word2vec_cbow_s50/cbow_s50.txt')\n",
    "word2vec_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/word2vec_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_bert(text)\n",
    "def text_to_embedding(text, model, vectorizer=None, vocab=None, size=50):\n",
    "    if not vectorizer:\n",
    "        raise Exception(\"The vectorizer parameter must not be None\")\n",
    "\n",
    "    transformed = vectorizer.transform(text)\n",
    "    vectorized = pd.DataFrame(transformed.toarray(\n",
    "    ), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    embeedings = pd.DataFrame()\n",
    "    for i in range(vectorized.shape[0]):\n",
    "        sentence = np.zeros(size)\n",
    "        for word in vocab[vectorized.iloc[i, :] > 0]:\n",
    "            if model.get_index(word, default=-1) != -1:\n",
    "                sentence = sentence + model.get_vector(word)\n",
    "            else:\n",
    "                print(\"Out of Vocabulary\")\n",
    "\n",
    "        embeedings = pd.concat([embeedings, pd.DataFrame([sentence])])\n",
    "\n",
    "    return embeedings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_names = [\"fasttext_cbow_s50\", \"fasttext_skip_s50\", \"glove_s50\",\n",
    "                   \"wang2vec_cbow_s50\", \"wang2vec_skip_s50\", \"word2vec_cbow_s50\", \"word2vec_skip_s50\"]\n",
    "embedding_models = [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n",
    "                    wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n",
    "\n",
    "for name, model in zip(embedding_names, embedding_models):\n",
    "    reviews_train_dtm = text_to_embedding(\n",
    "        reviews_train_dataset['review_text'], model, tv, reviews_test_processed_tv.columns[1:-1], 50)\n",
    "    reviews_train_processed = pd.concat([reviews_train_dataset.reset_index()[['original_index']], reviews_train_dtm.reset_index(\n",
    "        drop=True), reviews_train_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_train_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_train_dataset_{name}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorizer train dataframe has {reviews_train_processed.shape[0]} rows and {reviews_train_processed.shape[1]} columns\")\n",
    "\n",
    "    reviews_test_dtm = text_to_embedding(\n",
    "        reviews_test_dataset['review_text'], model, tv, reviews_test_processed_tv.columns[1:-1], 50)\n",
    "    reviews_test_processed = pd.concat([reviews_test_dataset.reset_index()[['original_index']], reviews_test_dtm.reset_index(\n",
    "        drop=True), reviews_test_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_test_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_test_dataset_{name}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorizer test dataframe has {reviews_test_processed.shape[0]} rows and {reviews_test_processed.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "print(f\"Transformers model class model: {type(model)}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "print(f\"Transformers tokenizer class: {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "# `encode_plus` will:\n",
    "#   (1) Tokenize the sentence.\n",
    "#   (2) Prepend the `[CLS]` token to the start.\n",
    "#   (3) Append the `[SEP]` token to the end.\n",
    "#   (4) Map tokens to their IDs.\n",
    "#   (5) Pad or truncate the sentence to `max_length`\n",
    "#   (6) Create attention masks for [PAD] tokens.\n",
    "text_train_encoded = reviews_train_dataset['review_text_cleaned'].apply(\n",
    "    lambda sentence:\n",
    "    tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=10,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "input_ids = [s['input_ids'] for s in text_train_encoded]\n",
    "attn_mask = [s['attention_mask'] for s in text_train_encoded]\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensors(descriptions, tokenizer, max_tokens=128):\n",
    "    # tokenization.\n",
    "    sentences = descriptions['review_text_cleaned'].apply(\n",
    "        (lambda s: ' '.join(s.split()[:max_tokens])))\n",
    "    tokenized = sentences.apply(\n",
    "        (lambda s: tokenizer.encode(s, add_special_tokens=True, truncation=True)))\n",
    "\n",
    "    # padding\n",
    "    max_len = max_tokens\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "    # masking\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    # model#1\n",
    "    input_ids = torch.tensor(padded)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def extract_features(dataset, model, tokenizer):\n",
    "    \n",
    "    bug_ids = dataset['original_index']\n",
    "\n",
    "    input_ids, attention_mask = build_tensors(dataset, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    features = last_hidden_states[0][:, 0, :].numpy()\n",
    "    \n",
    "    labels  = dataset['polarity']\n",
    "    \n",
    "    return (features, labels, bug_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "extract_features(reviews_test_dataset, model, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
