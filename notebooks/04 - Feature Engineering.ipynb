{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "RANDOM_SEED = 19730115\n",
    "NUMBER_OF_WORDS = 50\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "logging.info(\"Required packages installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, stratify=False):\n",
    "    \"\"\"Get the data from csv file\n",
    "\n",
    "    Args:\n",
    "        path(str): the file complete path. \n",
    "\n",
    "    Returns:\n",
    "        dataframe: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    if stratify:\n",
    "        dataset = dataset.groupby('polarity', group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=0.4))\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews datasets.\n",
    "reviews_train_dataset = load_dataset(\n",
    "    \"../data/processed/buscape_reviews_train_dataset.csv\", True)\n",
    "reviews_test_dataset = load_dataset(\n",
    "    \"../data/processed/buscape_reviews_test_dataset.csv\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Polarity Distribution in Train')\n",
    "reviews_train_dataset['polarity'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Polarity Distribution in Test')\n",
    "reviews_test_dataset['polarity'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS)\n",
    "reviews_train_cv = cv.fit_transform(\n",
    "    reviews_train_dataset['review_text_cleaned_no_stopwords'])\n",
    "reviews_train_dtm_cv = pd.DataFrame(\n",
    "    reviews_train_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "reviews_train_dtm_cv.index = reviews_train_dataset.index\n",
    "reviews_train_processed_cv = pd.concat([reviews_train_dataset[[\n",
    "                                       'original_index']], reviews_train_dtm_cv, reviews_train_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The counter vectorizer train matrix has {reviews_train_processed_cv.shape[0]} rows and {reviews_train_processed_cv.shape[1]} columns\")\n",
    "\n",
    "reviews_test_cv = cv.transform(\n",
    "    reviews_test_dataset['review_text_cleaned_no_stopwords'])\n",
    "reviews_test_dtm_cv = pd.DataFrame(\n",
    "    reviews_test_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "reviews_test_dtm_cv.index = reviews_test_dataset.index\n",
    "reviews_test_processed_cv = pd.concat([reviews_test_dataset[[\n",
    "                                      'original_index']], reviews_test_dtm_cv, reviews_test_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The counter vectorizer test matrix has {reviews_test_processed_cv.shape[0]} rows and {reviews_test_processed_cv.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_cv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_processed_cv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_cv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_train_dataset_cv_s{NUMBER_OF_WORDS}.pkl')\n",
    "reviews_test_processed_cv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_test_dataset_cv_s{NUMBER_OF_WORDS}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=50)\n",
    "reviews_train_tv = tv.fit_transform(reviews_train_dataset['review_text'])\n",
    "reviews_train_dtm_tv = pd.DataFrame(\n",
    "    reviews_train_tv.toarray(), columns=tv.get_feature_names_out())\n",
    "reviews_train_dtm_tv.index = reviews_train_dataset.index\n",
    "reviews_train_processed_tv = pd.concat([reviews_train_dataset[[\n",
    "                                       'original_index']], reviews_train_dtm_tv, reviews_train_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The tf-idf vectorizer train matrix has {reviews_train_processed_tv.shape[0]} rows and {reviews_train_processed_tv.shape[1]} columns\")\n",
    "\n",
    "reviews_test_tv = tv.transform(reviews_test_dataset['review_text'])\n",
    "reviews_test_dtm_tv = pd.DataFrame(\n",
    "    reviews_test_tv.toarray(), columns=tv.get_feature_names_out())\n",
    "reviews_test_dtm_tv.index = reviews_test_dataset.index\n",
    "reviews_test_processed_tv = pd.concat([reviews_test_dataset[[\n",
    "                                      'original_index']], reviews_test_dtm_tv, reviews_test_dataset[['polarity']]], axis=1)\n",
    "print(\n",
    "    f\"The tf-idf vectorizer test matrix has {reviews_test_processed_tv.shape[0]} rows and {reviews_test_processed_tv.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_tv.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_processed_tv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_train_dataset_tv_s{NUMBER_OF_WORDS}.pkl')\n",
    "reviews_test_processed_tv.to_pickle(\n",
    "    f'../data/processed/buscape_reviews_test_dataset_tv_s{NUMBER_OF_WORDS}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logging.info(\"Load fast text embeddings.\")\n",
    "fasttext_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/fasttext_cbow_s50/cbow_s50.txt')\n",
    "fasttext_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/fasttext_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned glove embedding.\n",
    "logging.info(\"Load glove embeddings.\")\n",
    "glove_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/glove_s50/glove_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logging.info(\"Load wang2vec embeddings.\")\n",
    "wang2vec_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/wang2vec_cbow_s50/cbow_s50.txt')\n",
    "wang2vec_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/wang2vec_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned word2vec embedding.\n",
    "logging.info(\"Load word2vec embeddings.\")\n",
    "word2vec_cbow_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/word2vec_cbow_s50/cbow_s50.txt')\n",
    "word2vec_skip_s50 = KeyedVectors.load_word2vec_format(\n",
    "    '../data/embeedings/word2vec_skip_s50/skip_s50.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_bert(text)\n",
    "def text_to_embedding(text, model, vectorizer=None, vocab=None, size=50):\n",
    "    if not vectorizer:\n",
    "        raise Exception(\"The vectorizer parameter must not be None\")\n",
    "\n",
    "    transformed = vectorizer.transform(text)\n",
    "    vectorized = pd.DataFrame(transformed.toarray(\n",
    "    ), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    embeedings = pd.DataFrame()\n",
    "    for i in range(vectorized.shape[0]):\n",
    "        sentence = np.zeros(size)\n",
    "        for word in vocab[vectorized.iloc[i, :] > 0]:\n",
    "            if model.get_index(word, default=-1) != -1:\n",
    "                sentence = sentence + model.get_vector(word)\n",
    "            else:\n",
    "                print(\"Out of Vocabulary\")\n",
    "\n",
    "        embeedings = pd.concat([embeedings, pd.DataFrame([sentence])])\n",
    "\n",
    "    return embeedings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_names = [\"fasttext_cbow_s50\", \"fasttext_skip_s50\", \"glove_s50\",\n",
    "                   \"wang2vec_cbow_s50\", \"wang2vec_skip_s50\", \"word2vec_cbow_s50\", \"word2vec_skip_s50\"]\n",
    "embedding_models = [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n",
    "                    wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n",
    "\n",
    "for name, model in zip(embedding_names, embedding_models):\n",
    "    reviews_train_dtm = text_to_embedding(\n",
    "        reviews_train_dataset['review_text'], model, tv, reviews_test_processed_tv.columns[1:-1], 50)\n",
    "    reviews_train_processed = pd.concat([reviews_train_dataset.reset_index()[['original_index']], reviews_train_dtm.reset_index(\n",
    "        drop=True), reviews_train_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_train_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_train_dataset_{name}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorizer train dataframe has {reviews_train_processed.shape[0]} rows and {reviews_train_processed.shape[1]} columns\")\n",
    "\n",
    "    reviews_test_dtm = text_to_embedding(\n",
    "        reviews_test_dataset['review_text'], model, tv, reviews_test_processed_tv.columns[1:-1], 50)\n",
    "    reviews_test_processed = pd.concat([reviews_test_dataset.reset_index()[['original_index']], reviews_test_dtm.reset_index(\n",
    "        drop=True), reviews_test_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_test_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_test_dataset_{name}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorizer test dataframe has {reviews_test_processed.shape[0]} rows and {reviews_test_processed.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "print(f\"Transformers model class model: {type(model)}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'neuralmind/bert-base-portuguese-cased', do_lower_case=True)\n",
    "print(f\"Transformers tokenizer class: {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "# `encode_plus` will:\n",
    "#   (1) Tokenize the sentence.\n",
    "#   (2) Prepend the `[CLS]` token to the start.\n",
    "#   (3) Append the `[SEP]` token to the end.\n",
    "#   (4) Map tokens to their IDs.\n",
    "#   (5) Pad or truncate the sentence to `max_length`\n",
    "#   (6) Create attention masks for [PAD] tokens.\n",
    "text_train_encoded = reviews_train_dataset['review_text_cleaned'].apply(\n",
    "    lambda sentence:\n",
    "    tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=10,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "input_ids = [s['input_ids'] for s in text_train_encoded]\n",
    "attn_mask = [s['attention_mask'] for s in text_train_encoded]\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensors(descriptions, tokenizer, max_tokens=128):\n",
    "    # tokenization.\n",
    "    sentences = descriptions['review_text_cleaned'].apply(\n",
    "        (lambda s: ' '.join(s.split()[:max_tokens])))\n",
    "    tokenized = sentences.apply(\n",
    "        (lambda s: tokenizer.encode(s, add_special_tokens=True, truncation=True)))\n",
    "\n",
    "    # padding\n",
    "    max_len = max_tokens\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "    # masking\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    # model#1\n",
    "    input_ids = torch.tensor(padded)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    return (input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def extract_features(dataset, model, tokenizer):\n",
    "    \n",
    "    bug_ids = dataset['original_index']\n",
    "\n",
    "    input_ids, attention_mask = build_tensors(dataset, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    features = last_hidden_states[0][:, 0, :].numpy()\n",
    "    \n",
    "    labels  = dataset['polarity']\n",
    "    \n",
    "    return (features, labels, bug_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "extract_features(reviews_test_dataset, model, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
