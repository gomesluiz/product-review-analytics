{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gomesluiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-12-03 12:37:32,452 - Required packages installed.\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "# Required packages.\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import string\n",
    "\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "RANDOM_SEED = 19730115\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.INFO)\n",
    "logger.info(\"Required packages installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script constants.\n",
    "DATA_ROOT_FOLDER = os.path.join(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__name__))), \"data\"\n",
    ")\n",
    "DATA_PROCESSED_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"processed\")\n",
    "DATA_EMBEDDINGS_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"embeddings\")\n",
    "URL_SOURCE = \"https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv\"\n",
    "RANDOM_SEED = 19730115\n",
    "NUMBER_OF_WORDS = 50\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripts Functions.\n",
    "def load_dataset(source) -> None:\n",
    "    \"\"\"Download data from a url.\n",
    "\n",
    "    Args:\n",
    "        source (str): source data file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_csv(source)\n",
    "\n",
    "\n",
    "def load_stratify_dataset(path, stratify=False):\n",
    "    \"\"\"Get the data from csv file\n",
    "\n",
    "    Args:\n",
    "        path(str): the file complete path.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    if stratify:\n",
    "        dataset = dataset.groupby(\"polarity\", group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=0.4)\n",
    "        )\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_pickle_dataset(path):\n",
    "    \"\"\"Read pickle.\n",
    "\n",
    "    Args:\n",
    "        path (str): The full dataset file.\n",
    "\n",
    "    Returns:\n",
    "        features(array) and target(array):\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the features and target variable.\n",
    "    data = pd.read_pickle(path)\n",
    "    features, target = data.iloc[:, 1:-1].values, data.iloc[:, -1].values\n",
    "\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def word_counter(text):\n",
    "    \"\"\"Word counter.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Make text lowercase, remove text in square brackets, remove punctuation and\n",
    "        remove words containing numbers.\n",
    "\n",
    "    Args:\n",
    "        text(str): string text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    text = re.sub('[``\"\"...]', \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_vector(model, dataset):\n",
    "\n",
    "    vectorizer = model.fit_transform(dataset[\"review_text_cleaned_no_stopwords\"])\n",
    "    vocab = model.get_feature_names_out()\n",
    "    dtm = pd.DataFrame(vectorizer.toarray(), columns=vocab)\n",
    "    dtm.index = dataset.index\n",
    "    return (\n",
    "        pd.concat(\n",
    "            [dataset[[\"original_index\"]], dtm, dataset[[\"polarity\"]]],\n",
    "            axis=1,\n",
    "        ),\n",
    "        vocab,\n",
    "    )\n",
    "\n",
    "\n",
    "# def text_to_bert(text)\n",
    "def text_to_embedding(text, model, vectorizer=None, vocab=None, size=50):\n",
    "    if not vectorizer:\n",
    "        raise Exception(\"The vectorizer parameter must not be None\")\n",
    "\n",
    "    transformed = vectorizer.transform(text)\n",
    "    vectorized = pd.DataFrame(\n",
    "        transformed.toarray(), columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    embeedings = pd.DataFrame()\n",
    "    for i in range(vectorized.shape[0]):\n",
    "        sentence = np.zeros(size)\n",
    "        for word in vocab[vectorized.iloc[i, :] > 0]:\n",
    "            if model.get_index(word, default=-1) != -1:\n",
    "                sentence = sentence + model.get_vector(word)\n",
    "            else:\n",
    "                print(\"Out of Vocabulary\")\n",
    "\n",
    "        embeedings = pd.concat([embeedings, pd.DataFrame([sentence])])\n",
    "\n",
    "    return embeedings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 12:38:11,483 - Dataset loaded from https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv.\n"
     ]
    }
   ],
   "source": [
    "reviews = load_dataset(URL_SOURCE)\n",
    "logger.info(f\"Dataset loaded from {URL_SOURCE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.dropna(subset=[\"review_text\"], inplace=True)\n",
    "reviews.loc[:, [\"review_text_cleaned\"]] = reviews[\"review_text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "reviews.loc[:, [\"review_text_cleaned_len\"]] = reviews[\"review_text_cleaned\"].apply(\n",
    "    word_counter\n",
    ")\n",
    "reviews.loc[:, [\"review_text_cleaned_no_stopwords\"]] = reviews[\n",
    "    \"review_text_cleaned\"\n",
    "].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords)]))\n",
    "reviews.loc[:, [\"review_text_cleaned_len_no_stopwords\"]] = reviews[\n",
    "    \"review_text_cleaned_no_stopwords\"\n",
    "].apply(word_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original polarity to -1 from 0, nan to 0.\n",
    "reviews_cleaned = reviews[\n",
    "    [\n",
    "        \"original_index\",\n",
    "        \"review_text\",\n",
    "        \"review_text_cleaned\",\n",
    "        \"review_text_cleaned_len\",\n",
    "        \"review_text_cleaned_no_stopwords\",\n",
    "        \"review_text_cleaned_len_no_stopwords\",\n",
    "        \"polarity\",\n",
    "    ]\n",
    "].copy()\n",
    "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].replace({0: -1, np.nan: 0})\n",
    "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].astype(int)\n",
    "#\n",
    "reviews_cleaned.dropna(subset=[\"review_text_cleaned_no_stopwords\"], inplace=True)\n",
    "reviews_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_dataset, reviews_test_dataset = train_test_split(\n",
    "    reviews_cleaned,\n",
    "    stratify=reviews_cleaned[\"polarity\"],\n",
    "    test_size=0.20,\n",
    "    random_state=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter vectorizer\n",
    "vectorizers = {\n",
    "    \"cv\": CountVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
    "    \"tv\": TfidfVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
    "}\n",
    "\n",
    "if not os.path.exists(DATA_PROCESSED_FOLDER):\n",
    "    os.makedirs(DATA_PROCESSED_FOLDER)\n",
    "\n",
    "for name, model in vectorizers.items():\n",
    "    reviews_train_vectorized, vocab = text_to_vector(model, reviews_train_dataset)\n",
    "    reviews_train_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_train_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
    "\n",
    "    reviews_test_vectorized, _ = text_to_vector(model, reviews_test_dataset)\n",
    "    reviews_test_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
    "    \n",
    "    logger.info(\n",
    "        f\"The {name} vectorizer train matrix has {reviews_train_vectorized.shape[0]} rows and {reviews_train_vectorized.shape[1]} columns\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"The {name} vectorizer test matrix has {reviews_test_vectorized.shape[0]} rows and {reviews_test_vectorized.shape[1]} columns\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "EMBEDDING_NAMES = [\n",
    "    [\"word2vec\", \"cbow_s50\"],\n",
    "    [\"word2vec\", \"skip_s50\"],\n",
    "    [\"fasttext\", \"cbow_s50\"],\n",
    "    [\"fasttext\", \"skip_s50\"],\n",
    "    [\"glove\", \"glove_s50\"],\n",
    "    [\"wang2vec\", \"cbow_s50\"],\n",
    "    [\"wang2vec\", \"skip_s50\"],\n",
    "]\n",
    "\n",
    "\n",
    "def download_extract(model, architecture):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    url = f\"http://143.107.183.175:22980/download.php?file=embeddings/{model}/{architecture}.zip\"\n",
    "    out_folder_path = os.path.join(DATA_EMBEDDINGS_FOLDER, model)\n",
    "    out_file_path = os.path.join(out_folder_path, architecture)\n",
    "    logger.info(f\"Downloading: {model}_{architecture}\")\n",
    "    if not os.path.exists(out_file_path):\n",
    "        with urlopen(url) as response:\n",
    "            with ZipFile(BytesIO(response.read())) as in_file_zip:\n",
    "                in_file_zip.extractall(out_folder_path)\n",
    "\n",
    "\n",
    "for model, architecture in EMBEDDING_NAMES:\n",
    "    download_extract(model, architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logger.info(\"Load fast text embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"fasttext\")\n",
    "fasttext_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "fasttext_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned glove embedding.\n",
    "logger.info(\"Load glove embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"glove\")\n",
    "glove_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER,\"glove_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned wang2vec embedding.\n",
    "logger.info(\"Load wang2vec embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"wang2vec\")\n",
    "wang2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "wang2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned word2vec embedding.\n",
    "logger.info(\"Load word2vec embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"word2vec\")\n",
    "word2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "word2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n",
    "                    wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n",
    "\n",
    "for name, model in zip(EMBEDDING_NAMES, embedding_models):\n",
    "    reviews_train_dtm = text_to_embedding(\n",
    "        reviews_train_dataset['review_text'], model, vectorizers[\"tv\"], vocab, 50)\n",
    "    reviews_train_processed = pd.concat([reviews_train_dataset.reset_index()[['original_index']], reviews_train_dtm.reset_index(\n",
    "        drop=True), reviews_train_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_train_processed.to_pickle(\n",
    "        os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_train_dataset_{name[0]}_{name[1]}.pkl\"))\n",
    "    logger.info(\n",
    "        f\"The {name} vectorized train dataframe has {reviews_train_processed.shape[0]} rows and {reviews_train_processed.shape[1]} columns\")\n",
    "\n",
    "    reviews_test_dtm = text_to_embedding(\n",
    "        reviews_test_dataset['review_text'], model, vectorizers[\"tv\"], vocab, 50)\n",
    "    reviews_test_processed = pd.concat([reviews_test_dataset.reset_index()[['original_index']], reviews_test_dtm.reset_index(\n",
    "        drop=True), reviews_test_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_test_processed.to_pickle(\n",
    "        os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name[0]}_{name[1]}.pkl\"))\n",
    "    logger.info(\n",
    "        f\"The {name} vectorized test dataframe has {reviews_test_processed.shape[0]} rows and {reviews_test_processed.shape[1]} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d8fcf32222123c2bc016e08cf8007b014ada14ae08852eb2d821271c6218457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
