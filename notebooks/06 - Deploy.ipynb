{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6T-U2o2vJ_s-",
        "outputId": "2a1b7bef-4070-45ac-89f3-0e1b0de2a0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting asttokens==2.2.0\n",
            "  Downloading asttokens-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 2)) (0.2.0)\n",
            "Collecting black==22.10.0\n",
            "  Downloading black-22.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis==0.7.9 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 4)) (0.7.9)\n",
            "Requirement already satisfied: catalogue==2.0.8 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 5)) (2.0.8)\n",
            "Requirement already satisfied: certifi==2022.9.24 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 6)) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 7)) (2.1.1)\n",
            "Collecting click==8.1.3\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.6\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: confection==0.0.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 10)) (0.0.3)\n",
            "Collecting contourpy==1.0.6\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 92.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 12)) (0.11.0)\n",
            "Requirement already satisfied: cymem==2.0.7 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 13)) (2.0.7)\n",
            "Collecting debugpy==1.6.4\n",
            "  Downloading debugpy-1.6.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 67.6 MB/s \n",
            "\u001b[?25hCollecting decorator==5.1.1\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 16)) (0.4)\n",
            "Collecting executing==1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: filelock==3.8.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 18)) (3.8.0)\n",
            "Collecting fonttools==4.38.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting gensim==4.2.0\n",
            "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 64.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.11.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 94.5 MB/s \n",
            "\u001b[?25hCollecting idna==3.4\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 117 kB/s \n",
            "\u001b[?25hCollecting ipykernel==6.17.1\n",
            "  Downloading ipykernel-6.17.1-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 89.9 MB/s \n",
            "\u001b[?25hCollecting ipython==8.7.0\n",
            "  Downloading ipython-8.7.0-py3-none-any.whl (761 kB)\n",
            "\u001b[K     |████████████████████████████████| 761 kB 81.4 MB/s \n",
            "\u001b[?25hCollecting jedi==0.18.2\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 85.5 MB/s \n",
            "\u001b[?25hCollecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 97.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 27)) (1.2.0)\n",
            "Collecting jupyter_client==7.4.7\n",
            "  Downloading jupyter_client-7.4.7-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 88.8 MB/s \n",
            "\u001b[?25hCollecting jupyter_core==5.1.0\n",
            "  Downloading jupyter_core-5.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 982 kB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 30)) (1.4.4)\n",
            "Requirement already satisfied: langcodes==3.3.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 31)) (3.3.0)\n",
            "Collecting MarkupSafe==2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting matplotlib==3.6.2\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 96.9 MB/s \n",
            "\u001b[?25hCollecting matplotlib-inline==0.1.6\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: murmurhash==1.0.9 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 35)) (1.0.9)\n",
            "Collecting mypy-extensions==0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting nest-asyncio==1.5.6\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: nltk==3.7 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 38)) (3.7)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging==21.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 40)) (21.3)\n",
            "Collecting pandas==1.5.2\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2 MB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 42)) (0.8.3)\n",
            "Collecting pathspec==0.10.2\n",
            "  Downloading pathspec-0.10.2-py3-none-any.whl (28 kB)\n",
            "Collecting pathy==0.8.1\n",
            "  Downloading pathy-0.8.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 45)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 46)) (0.7.5)\n",
            "Collecting Pillow==9.3.0\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 86.8 MB/s \n",
            "\u001b[?25hCollecting platformdirs==2.5.4\n",
            "  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: preshed==3.0.8 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 49)) (3.0.8)\n",
            "Collecting prompt-toolkit==3.0.33\n",
            "  Downloading prompt_toolkit-3.0.33-py3-none-any.whl (383 kB)\n",
            "\u001b[K     |████████████████████████████████| 383 kB 97.1 MB/s \n",
            "\u001b[?25hCollecting psutil==5.9.4\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 101.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 52)) (0.7.0)\n",
            "Collecting pure-eval==0.2.2\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pydantic==1.10.2 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 54)) (1.10.2)\n",
            "Collecting Pygments==2.13.0\n",
            "  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 87.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.0.9 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 56)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 57)) (2.8.2)\n",
            "Requirement already satisfied: pytz==2022.6 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 58)) (2022.6)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 59)) (6.0)\n",
            "Collecting pyzmq==24.0.1\n",
            "  Downloading pyzmq-24.0.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 85.2 MB/s \n",
            "\u001b[?25hCollecting regex==2022.10.31\n",
            "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[K     |████████████████████████████████| 772 kB 76.0 MB/s \n",
            "\u001b[?25hCollecting requests==2.28.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 140.5 MB/s \n",
            "\u001b[?25hCollecting scipy==1.9.3\n",
            "  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 33.8 MB 12.1 MB/s \n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: smart-open==5.2.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 66)) (5.2.1)\n",
            "Requirement already satisfied: spacy==3.4.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 67)) (3.4.3)\n",
            "Requirement already satisfied: spacy-legacy==3.0.10 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 68)) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers==1.0.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 69)) (1.0.3)\n",
            "Requirement already satisfied: srsly==2.4.5 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 70)) (2.4.5)\n",
            "Collecting stack-data==0.6.2\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: thinc==8.1.5 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 72)) (8.1.5)\n",
            "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 73)) (3.1.0)\n",
            "Collecting tokenizers==0.13.2\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 93.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 75)) (2.0.1)\n",
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 1.1 MB/s eta 0:00:51tcmalloc: large alloc 1147494400 bytes == 0x3a292000 @  0x7f8d1a71b615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |████████████████████████████████| 890.2 MB 5.5 kB/s \n",
            "\u001b[?25hCollecting tornado==6.2\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 78)) (4.64.1)\n",
            "Collecting traitlets==5.6.0\n",
            "  Downloading traitlets-5.6.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 99.6 MB/s \n",
            "\u001b[?25hCollecting transformers==4.24.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 87.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 81)) (0.7.0)\n",
            "Collecting typing_extensions==4.4.0\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting urllib3==1.26.12\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 94.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi==0.10.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 84)) (0.10.1)\n",
            "Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 85)) (0.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==3.4.3->-r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 67)) (57.4.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 41 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 12 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 100.4 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 91.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt (line 76)) (0.38.4)\n",
            "Installing collected packages: typing-extensions, six, urllib3, traitlets, pure-eval, platformdirs, numpy, idna, executing, click, asttokens, tornado, stack-data, requests, pyzmq, Pygments, prompt-toolkit, nvidia-cublas-cu11, nest-asyncio, matplotlib-inline, MarkupSafe, jupyter-core, jedi, decorator, tokenizers, scipy, regex, psutil, Pillow, pathy, pathspec, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, mypy-extensions, jupyter-client, Jinja2, ipython, huggingface-hub, fonttools, debugpy, contourpy, transformers, torch, scikit-learn, pandas, matplotlib, ipykernel, gensim, colorama, black\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.0.4\n",
            "    Uninstalling tornado-6.0.4:\n",
            "      Successfully uninstalled tornado-6.0.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 23.2.1\n",
            "    Uninstalling pyzmq-23.2.1:\n",
            "      Successfully uninstalled pyzmq-23.2.1\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 2.0.10\n",
            "    Uninstalling prompt-toolkit-2.0.10:\n",
            "      Successfully uninstalled prompt-toolkit-2.0.10\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter-core 4.11.2\n",
            "    Uninstalling jupyter-core-4.11.2:\n",
            "      Successfully uninstalled jupyter-core-4.11.2\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: pathy\n",
            "    Found existing installation: pathy 0.9.0\n",
            "    Uninstalling pathy-0.9.0:\n",
            "      Successfully uninstalled pathy-0.9.0\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "  Attempting uninstall: debugpy\n",
            "    Found existing installation: debugpy 1.0.0\n",
            "    Uninstalling debugpy-1.0.0:\n",
            "      Successfully uninstalled debugpy-1.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.3.4\n",
            "    Uninstalling ipykernel-5.3.4:\n",
            "      Successfully uninstalled ipykernel-5.3.4\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
            "notebook 5.7.16 requires jupyter-client<7.0.0,>=5.2.0, but you have jupyter-client 7.4.7 which is incompatible.\n",
            "moviepy 0.2.3.5 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=5.3.4, but you have ipykernel 6.17.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.7.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Pillow-9.3.0 Pygments-2.13.0 asttokens-2.2.0 black-22.10.0 click-8.1.3 colorama-0.4.6 contourpy-1.0.6 debugpy-1.6.4 decorator-5.1.1 executing-1.2.0 fonttools-4.38.0 gensim-4.2.0 huggingface-hub-0.11.0 idna-3.4 ipykernel-6.17.1 ipython-8.7.0 jedi-0.18.2 jupyter-client-7.4.7 jupyter-core-5.1.0 matplotlib-3.6.2 matplotlib-inline-0.1.6 mypy-extensions-0.4.3 nest-asyncio-1.5.6 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.2 pathspec-0.10.2 pathy-0.8.1 platformdirs-2.5.4 prompt-toolkit-3.0.33 psutil-5.9.4 pure-eval-0.2.2 pyzmq-24.0.1 regex-2022.10.31 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 six-1.16.0 stack-data-0.6.2 tokenizers-0.13.2 torch-1.13.0 tornado-6.2 traitlets-5.6.0 transformers-4.24.0 typing-extensions-4.4.0 urllib3-1.26.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "PIL",
                  "debugpy",
                  "decorator",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "prompt_toolkit",
                  "psutil",
                  "pygments",
                  "six",
                  "torch",
                  "tornado",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install requirement packages.\n",
        "%pip install -r https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qze6drD8J_tB",
        "outputId": "c4d2f479-91b2-4ae5-f491-23a250944922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required packages installed.\n"
          ]
        }
      ],
      "source": [
        "# Required packages.\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import logging\n",
        "import string\n",
        "\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "print(\"Required packages installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vAPtJ9V0J_tD"
      },
      "outputs": [],
      "source": [
        "# Script constants.\n",
        "DATA_ROOT_FOLDER = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(__name__))), \"data\"\n",
        ")\n",
        "DATA_PROCESSED_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"processed\")\n",
        "DATA_EMBEDDINGS_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"embeddings\")\n",
        "URL_SOURCE = \"https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv\"\n",
        "RANDOM_SEED = 19730115\n",
        "NUMBER_OF_WORDS = 50\n",
        "rng = np.random.RandomState(RANDOM_SEED)\n",
        "\n",
        "EMBEDDING_NAMES = [\n",
        "    [\"word2vec\", \"cbow_s50\"],\n",
        "    [\"word2vec\", \"skip_s50\"],\n",
        "    [\"fasttext\", \"cbow_s50\"],\n",
        "    [\"fasttext\", \"skip_s50\"],\n",
        "    [\"glove\", \"glove_s50\"],\n",
        "    [\"wang2vec\", \"cbow_s50\"],\n",
        "    [\"wang2vec\", \"skip_s50\"],\n",
        "]\n",
        "\n",
        "# Vectorizer model names.\n",
        "VECTORIZER_NAMES = [\n",
        "    \"cv_s50\",\n",
        "    \"tv_s50\",\n",
        "    \"fasttext_cbow_s50\",\n",
        "    \"fasttext_skip_s50\",\n",
        "    \"glove_s50\",\n",
        "    \"wang2vec_cbow_s50\",\n",
        "    \"wang2vec_skip_s50\",\n",
        "    \"word2vec_cbow_s50\",\n",
        "    \"word2vec_skip_s50\",\n",
        "    \"bert\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stopwords = nltk.corpus.stopwords.words(\"portuguese\")"
      ],
      "metadata": {
        "id": "udlIZYF7sXn0",
        "outputId": "6bde713d-94db-4bd7-955f-f0ff9f88a165",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "juGfaK3iJ_tE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Common Functions.\n",
        "def load_dataset(path, frac=None):\n",
        "    \"\"\"Get the data from csv file\n",
        "\n",
        "    Args:\n",
        "        path(str): the file complete path.\n",
        "\n",
        "    Returns:\n",
        "        dataframe: A pandas dataframe.\n",
        "    \"\"\"\n",
        "    dataset = pd.read_csv(path)\n",
        "\n",
        "    if frac:\n",
        "        dataset = dataset.groupby(\"polarity\", group_keys=False).apply(\n",
        "            lambda x: x.sample(frac=0.4, random_state=rng)\n",
        "        )\n",
        "        dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_pickle_dataset(path):\n",
        "    \"\"\"Read pickle.\n",
        "\n",
        "    Args:\n",
        "        path (str): The full dataset file.\n",
        "\n",
        "    Returns:\n",
        "        features(array) and target(array):\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the features and target variable.\n",
        "    data = pd.read_pickle(path)\n",
        "    features, target = data.iloc[:, 1:-1].values, data.iloc[:, -1].values\n",
        "\n",
        "    return features, target\n",
        "\n",
        "\n",
        "def count_word(text):\n",
        "    \"\"\"Word counter.\"\"\"\n",
        "    return len(text.split())\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Make text lowercase, remove text in square brackets, remove punctuation and\n",
        "        remove words containing numbers.\n",
        "\n",
        "    Args:\n",
        "        text(str): string text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned text\n",
        "\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
        "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
        "    text = re.sub('[``\"\"...]', \"\", text)\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_to_vector(model, dataset):\n",
        "\n",
        "    vectorizer = model.fit_transform(dataset[\"review_text_cleaned_no_stopwords\"])\n",
        "    vocab = model.get_feature_names_out()\n",
        "    dtm = pd.DataFrame(vectorizer.toarray(), columns=vocab)\n",
        "    dtm.index = dataset.index\n",
        "    return (\n",
        "        pd.concat(\n",
        "            [dataset[[\"original_index\"]], dtm, dataset[[\"polarity\"]]],\n",
        "            axis=1,\n",
        "        ),\n",
        "        vocab,\n",
        "    )\n",
        "\n",
        "\n",
        "# def text_to_bert(text)\n",
        "def text_to_embedding(text, model, vectorizer=None, vocab=None, size=50):\n",
        "    if not vectorizer:\n",
        "        raise Exception(\"The vectorizer parameter must not be None\")\n",
        "\n",
        "    transformed = vectorizer.transform(text)\n",
        "    vectorized = pd.DataFrame(\n",
        "        transformed.toarray(), columns=vectorizer.get_feature_names_out()\n",
        "    )\n",
        "\n",
        "    embeedings = pd.DataFrame()\n",
        "    for i in range(vectorized.shape[0]):\n",
        "        sentence = np.zeros(size)\n",
        "        for word in vocab[vectorized.iloc[i, :] > 0]:\n",
        "            if model.get_index(word, default=-1) != -1:\n",
        "                sentence = sentence + model.get_vector(word)\n",
        "            else:\n",
        "                print(\"Out of Vocabulary\")\n",
        "\n",
        "        embeedings = pd.concat([embeedings, pd.DataFrame([sentence])])\n",
        "\n",
        "    return embeedings\n",
        "\n",
        "def download_extract(model, architecture):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    url = f\"http://143.107.183.175:22980/download.php?file=embeddings/{model}/{architecture}.zip\"\n",
        "    out_folder_path = os.path.join(DATA_EMBEDDINGS_FOLDER, model)\n",
        "    out_file_path = os.path.join(out_folder_path, architecture)\n",
        "    print(f\"Downloading: {model}_{architecture}\")\n",
        "    if not os.path.exists(out_file_path):\n",
        "        with urlopen(url) as response:\n",
        "            with ZipFile(BytesIO(response.read())) as in_file_zip:\n",
        "                in_file_zip.extractall(out_folder_path)\n",
        "\n",
        "def train_model(X, y, estimator=None, distributions=None):\n",
        "    \"\"\"Split the dataframe in training and testing partitions.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        X (Numpy array):\n",
        "        y (Numpy array):\n",
        "        estimator (Sklearn model):\n",
        "        distributions (dict):\n",
        "\n",
        "    Returns:\n",
        "        Best estimator, mean test score and\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    kfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=rng)\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_distributions=distributions,\n",
        "        scoring=\"f1_micro\",\n",
        "        refit=True,\n",
        "        verbose=0,\n",
        "        random_state=rng,\n",
        "    )\n",
        "\n",
        "    model = search.fit(X, y)\n",
        "    return (\n",
        "        model.best_estimator_,\n",
        "        model.cv_results_[\"mean_test_score\"],\n",
        "        model.cv_results_[\"mean_score_time\"],\n",
        "    )\n",
        "\n",
        "\n",
        "def test_model(model, X_test, y_test):\n",
        "    \"\"\"Test model.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    return f1_score(y_pred, y_test, average=\"micro\")\n",
        "\n",
        "\n",
        "def train_test_model(vectorizer_names, estimator, estimator_distributions):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    train_scores, train_times, test_scores = [], [], []\n",
        "    for name in vectorizer_names:\n",
        "        logger.info(f\"Load {name} vectorized database.\")\n",
        "        reviews_train_features, reviews_train_target = load_pickle_dataset(\n",
        "            f\"../data/processed/buscape_reviews_train_dataset_{name}.pkl\"\n",
        "        )\n",
        "        reviews_test_features, reviews_test_target = load_pickle_dataset(\n",
        "            f\"../data/processed/buscape_reviews_test_dataset_{name}.pkl\"\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Train {name} with vectorized database.\")\n",
        "        model, scores, times = train_model(\n",
        "            reviews_train_features,\n",
        "            reviews_train_target,\n",
        "            estimator,\n",
        "            estimator_distributions,\n",
        "        )\n",
        "        train_scores.append(list(scores))\n",
        "        train_times.append(list(times))\n",
        "\n",
        "        logger.info(f\"Test {name} with vectorized database.\")\n",
        "        test_scores.append(\n",
        "            test_model(model, reviews_test_features, reviews_test_target)\n",
        "        )\n",
        "\n",
        "    return train_scores, train_times, test_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare dataset"
      ],
      "metadata": {
        "id": "naiVO-PcOZ6J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WFmrKSYJ_tG",
        "outputId": "5ac87fd7-7766-4dff-a3fd-02454541a324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviews dataset loaded from https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv.\n",
            "The reviews dataset has 29451 rows and 8 cols.\n"
          ]
        }
      ],
      "source": [
        "reviews = load_dataset(URL_SOURCE, frac=0.4)\n",
        "print(f\"The reviews dataset loaded from {URL_SOURCE}.\")\n",
        "print(f\"The reviews dataset has {reviews.shape[0]} rows and {reviews.shape[1]} cols.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "qj-SSznvJ_tG",
        "outputId": "601ded6d-897e-43e8-e24a-cad39c35f1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-7b51556cd60f>:7: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  reviews.loc[:, [\"review_text_cleaned_len\"]] = reviews[\"review_text_cleaned\"].apply(\n",
            "<ipython-input-8-7b51556cd60f>:14: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  reviews.loc[:, [\"review_text_cleaned_len_no_stopwords\"]] = reviews[\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   original_index                                        review_text  \\\n",
              "0        0_430974  Dá muito problema no encanamento e faz muito b...   \n",
              "1         0_43825  poow galera aki naum da para upa direito por q...   \n",
              "2        0_401867  Não gostei deste aparelho. Não dá pra deixar o...   \n",
              "3        9_159935  Muito ruim a adega acd 28 pois o compartimento...   \n",
              "4  minus_1_393969  A tv LCD é muito boa e economiza espaço dentro...   \n",
              "\n",
              "                                 review_text_cleaned  review_text_cleaned_len  \\\n",
              "0  dá muito problema no encanamento e faz muito b...                        9   \n",
              "1  poow galera aki naum da para upa direito por q...                       28   \n",
              "2  não gostei deste aparelho não dá pra deixar os...                       24   \n",
              "3  muito ruim a adega acd  pois o compartimento i...                       79   \n",
              "4  a tv lcd é muito boa e economiza espaço dentro...                       23   \n",
              "\n",
              "                    review_text_cleaned_no_stopwords  \\\n",
              "0                dá problema encanamento faz barulho   \n",
              "1  poow galera aki naum upa direito muita gente f...   \n",
              "2  gostei deste aparelho dá pra deixar aplicativo...   \n",
              "3  ruim adega acd pois compartimento inferior alt...   \n",
              "4  tv lcd boa economiza espaço dentro casa gostei...   \n",
              "\n",
              "   review_text_cleaned_len_no_stopwords  polarity  \n",
              "0                                     5        -1  \n",
              "1                                    14        -1  \n",
              "2                                    16        -1  \n",
              "3                                    46        -1  \n",
              "4                                    13        -1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55f682fe-9dd6-4b32-9222-381b208c6b57\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_index</th>\n",
              "      <th>review_text</th>\n",
              "      <th>review_text_cleaned</th>\n",
              "      <th>review_text_cleaned_len</th>\n",
              "      <th>review_text_cleaned_no_stopwords</th>\n",
              "      <th>review_text_cleaned_len_no_stopwords</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_430974</td>\n",
              "      <td>Dá muito problema no encanamento e faz muito b...</td>\n",
              "      <td>dá muito problema no encanamento e faz muito b...</td>\n",
              "      <td>9</td>\n",
              "      <td>dá problema encanamento faz barulho</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_43825</td>\n",
              "      <td>poow galera aki naum da para upa direito por q...</td>\n",
              "      <td>poow galera aki naum da para upa direito por q...</td>\n",
              "      <td>28</td>\n",
              "      <td>poow galera aki naum upa direito muita gente f...</td>\n",
              "      <td>14</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_401867</td>\n",
              "      <td>Não gostei deste aparelho. Não dá pra deixar o...</td>\n",
              "      <td>não gostei deste aparelho não dá pra deixar os...</td>\n",
              "      <td>24</td>\n",
              "      <td>gostei deste aparelho dá pra deixar aplicativo...</td>\n",
              "      <td>16</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9_159935</td>\n",
              "      <td>Muito ruim a adega acd 28 pois o compartimento...</td>\n",
              "      <td>muito ruim a adega acd  pois o compartimento i...</td>\n",
              "      <td>79</td>\n",
              "      <td>ruim adega acd pois compartimento inferior alt...</td>\n",
              "      <td>46</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>minus_1_393969</td>\n",
              "      <td>A tv LCD é muito boa e economiza espaço dentro...</td>\n",
              "      <td>a tv lcd é muito boa e economiza espaço dentro...</td>\n",
              "      <td>23</td>\n",
              "      <td>tv lcd boa economiza espaço dentro casa gostei...</td>\n",
              "      <td>13</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55f682fe-9dd6-4b32-9222-381b208c6b57')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55f682fe-9dd6-4b32-9222-381b208c6b57 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55f682fe-9dd6-4b32-9222-381b208c6b57');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Clean dataset and collect text statistics.\n",
        "reviews.dropna(subset=[\"review_text\"], inplace=True)\n",
        "reviews.loc[:, [\"review_text_cleaned\"]] = reviews[\"review_text\"].apply(\n",
        "    lambda x: clean_text(x)\n",
        ")\n",
        "\n",
        "reviews.loc[:, [\"review_text_cleaned_len\"]] = reviews[\"review_text_cleaned\"].apply(\n",
        "    count_word\n",
        ")\n",
        "reviews.loc[:, [\"review_text_cleaned_no_stopwords\"]] = reviews[\n",
        "    \"review_text_cleaned\"\n",
        "].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords)]))\n",
        "\n",
        "reviews.loc[:, [\"review_text_cleaned_len_no_stopwords\"]] = reviews[\n",
        "    \"review_text_cleaned_no_stopwords\"\n",
        "].apply(count_word)\n",
        "\n",
        "# Replace the original polarity to -1 from 0, nan to 0.\n",
        "reviews_cleaned = reviews[\n",
        "    [\n",
        "        \"original_index\",\n",
        "        \"review_text\",\n",
        "        \"review_text_cleaned\",\n",
        "        \"review_text_cleaned_len\",\n",
        "        \"review_text_cleaned_no_stopwords\",\n",
        "        \"review_text_cleaned_len_no_stopwords\",\n",
        "        \"polarity\",\n",
        "    ]\n",
        "].copy()\n",
        "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].replace({0: -1, np.nan: 0})\n",
        "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].astype(int)\n",
        "#\n",
        "reviews_cleaned.dropna(subset=[\"review_text_cleaned_no_stopwords\"], inplace=True)\n",
        "reviews_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorize dataset"
      ],
      "metadata": {
        "id": "n_b4vnzxQF7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HGh1zm2FJ_tI"
      },
      "outputs": [],
      "source": [
        "reviews_train_dataset, reviews_test_dataset = train_test_split(\n",
        "    reviews_cleaned,\n",
        "    stratify=reviews_cleaned[\"polarity\"],\n",
        "    test_size=0.20,\n",
        "    random_state=rng,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFYlkmEJ_tI",
        "outputId": "c0935b2c-aca7-428d-8e55-8e1242b382b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cv vectorizer train matrix has 23560 rows and 52 columns\n",
            "The cv vectorizer test matrix has 5891 rows and 52 columns\n",
            "The tv vectorizer train matrix has 23560 rows and 52 columns\n",
            "The tv vectorizer test matrix has 5891 rows and 52 columns\n"
          ]
        }
      ],
      "source": [
        "# Counter vectorizer\n",
        "vectorizers = {\n",
        "    \"cv\": CountVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
        "    \"tv\": TfidfVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
        "}\n",
        "\n",
        "if not os.path.exists(DATA_PROCESSED_FOLDER):\n",
        "    os.makedirs(DATA_PROCESSED_FOLDER)\n",
        "\n",
        "for name, model in vectorizers.items():\n",
        "    reviews_train_vectorized, vocab = text_to_vector(model, reviews_train_dataset)\n",
        "    reviews_train_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_train_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
        "\n",
        "    reviews_test_vectorized, _ = text_to_vector(model, reviews_test_dataset)\n",
        "    reviews_test_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
        "    \n",
        "    print(\n",
        "        f\"The {name} vectorizer train matrix has {reviews_train_vectorized.shape[0]} rows and {reviews_train_vectorized.shape[1]} columns\"\n",
        "    )\n",
        "    print(\n",
        "        f\"The {name} vectorizer test matrix has {reviews_test_vectorized.shape[0]} rows and {reviews_test_vectorized.shape[1]} columns\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZrbz__gJ_tJ",
        "outputId": "10309866-5119-4bad-f463-06323136992a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: word2vec_cbow_s50\n",
            "Downloading: word2vec_skip_s50\n",
            "Downloading: fasttext_cbow_s50\n",
            "Downloading: fasttext_skip_s50\n",
            "Downloading: glove_glove_s50\n",
            "Downloading: wang2vec_cbow_s50\n",
            "Downloading: wang2vec_skip_s50\n"
          ]
        }
      ],
      "source": [
        "# Download embeddings model.\n",
        "for model, architecture in EMBEDDING_NAMES:\n",
        "    download_extract(model, architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO9AKmkGJ_tK",
        "outputId": "0ba50061-2f27-4fbc-85bf-a9143a2a1fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load fast text embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trainned fast text embedding.\n",
        "print(\"Load fast text embeddings.\")\n",
        "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"fasttext\")\n",
        "fasttext_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
        "fasttext_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07YZRPurJ_tK",
        "outputId": "01d742a7-60db-47f1-f917-7939a91c5dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load glove embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trainned glove embedding.\n",
        "print(\"Load glove embeddings.\")\n",
        "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"glove\")\n",
        "glove_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER,\"glove_s50.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oW8mkSKJ_tL",
        "outputId": "5f1c7561-0e94-49ea-b73c-967cc1fd8c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load wang2vec embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trainned wang2vec embedding.\n",
        "print(\"Load wang2vec embeddings.\")\n",
        "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"wang2vec\")\n",
        "wang2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
        "wang2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kIn7gPJ_tM",
        "outputId": "8438d486-96e3-438c-c760-501d80d657b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load word2vec embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trainned word2vec embedding.\n",
        "print(\"Load word2vec embeddings.\")\n",
        "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"word2vec\")\n",
        "word2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
        "word2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P2Y1FCMJ_tM",
        "outputId": "0cb987a7-edb2-4a8f-ad08-f11c999623b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"The ['word2vec', 'cbow_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['word2vec', 'cbow_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['word2vec', 'skip_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['word2vec', 'skip_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['fasttext', 'cbow_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['fasttext', 'cbow_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['fasttext', 'skip_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['fasttext', 'skip_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['glove', 'glove_s50'] vectorized train dataframe has 23560 rows and 52 \"\n",
            " 'columns')\n",
            "The ['glove', 'glove_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['wang2vec', 'cbow_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['wang2vec', 'cbow_s50'] vectorized test dataframe has 5891 rows and 52 columns\n",
            "(\"The ['wang2vec', 'skip_s50'] vectorized train dataframe has 23560 rows and \"\n",
            " '52 columns')\n",
            "The ['wang2vec', 'skip_s50'] vectorized test dataframe has 5891 rows and 52 columns\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import pprint\n",
        "embedding_models = [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n",
        "                    wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n",
        "\n",
        "for name, model in zip(EMBEDDING_NAMES, embedding_models):\n",
        "    reviews_train_dtm = text_to_embedding(\n",
        "        reviews_train_dataset['review_text'], model, vectorizers[\"tv\"], vocab, 50)\n",
        "    reviews_train_processed = pd.concat([reviews_train_dataset.reset_index()[['original_index']], reviews_train_dtm.reset_index(\n",
        "        drop=True), reviews_train_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
        "    reviews_train_processed.to_pickle(\n",
        "        os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_train_dataset_{name[0]}_{name[1]}.pkl\"))\n",
        "    pprint(\n",
        "        f\"The {name} vectorized train dataframe has {reviews_train_processed.shape[0]} rows and {reviews_train_processed.shape[1]} columns\")\n",
        "\n",
        "    reviews_test_dtm = text_to_embedding(\n",
        "        reviews_test_dataset['review_text'], model, vectorizers[\"tv\"], vocab, 50)\n",
        "    reviews_test_processed = pd.concat([reviews_test_dataset.reset_index()[['original_index']], reviews_test_dtm.reset_index(\n",
        "        drop=True), reviews_test_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
        "    reviews_test_processed.to_pickle(\n",
        "        os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name[0]}_{name[1]}.pkl\"))\n",
        "    print(\n",
        "        f\"The {name} vectorized test dataframe has {reviews_test_processed.shape[0]} rows and {reviews_test_processed.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qcujluc2J_tN",
        "outputId": "76a860c8-ee76-49a3-f921-3e7d3e9d42c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7baf5832d14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "logging.info(f\"Transformers model class model: {type(model)}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"neuralmind/bert-base-portuguese-cased\", do_lower_case=True\n",
        ")\n",
        "logging.info(f\"Transformers tokenizer class: {type(tokenizer)}\")\n",
        "\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,     # Add `[CLS]` and `[SEP]`\n",
        "            max_length=64,               # Max length to truncate/pad\n",
        "            padding='max_length',        # Pad sentence to max length\n",
        "            truncation='only_first',     # Truncate sentence to max length\n",
        "            return_attention_mask=True,  # Return attention mask\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get(\"input_ids\"))\n",
        "        attention_masks.append(encoded_sent.get(\"attention_mask\"))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "\n",
        "print(\"Embedding train dataset.\")\n",
        "review_train_inputs, review_train_masks = preprocessing_for_bert(\n",
        "    list(reviews_train_dataset[\"review_text\"])\n",
        ")\n",
        "with torch.no_grad():\n",
        "    outs = model(review_train_inputs, review_train_masks)\n",
        "    review_train_bert_encoded = outs[0][:, 0, :]\n",
        "\n",
        "print(\"Embedding test dataset.\")\n",
        "review_test_inputs, review_test_masks = preprocessing_for_bert(\n",
        "    list(reviews_test_dataset[\"review_text\"])\n",
        ")\n",
        "with torch.no_grad():\n",
        "    outs = model(review_test_inputs, review_test_masks)\n",
        "    review_test_bert_encoded = outs[0][:, 0, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rnpCCQwJ_tN"
      },
      "outputs": [],
      "source": [
        "reviews_train_processed_bert = np.column_stack(\n",
        "    (\n",
        "        reviews_train_dataset[[\"original_index\"]],\n",
        "        review_train_bert_encoded,\n",
        "        reviews_train_dataset[[\"polarity\"]],\n",
        "    )\n",
        ")\n",
        "reviews_train_processed_bert.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, \"buscape_reviews_train_dataset_bert.pkl\"))\n",
        "\n",
        "reviews_test_processed_bert = np.column_stack(\n",
        "    (\n",
        "        reviews_test_dataset[[\"original_index\"]],\n",
        "        review_test_bert_encoded,\n",
        "        reviews_test_dataset[[\"polarity\"]],\n",
        "    )\n",
        ")\n",
        "reviews_test_processed_bert.to_pickle(os.path.join(DATA_PROCESSED_FOLDER,\"buscape_reviews_train_dataset_bert.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://scikit-learn.org/stable/modules/grid_search.html.\n",
        "knn_estimator = KNeighborsClassifier()\n",
        "knn_estimator_distributions = {\n",
        "    \"n_neighbors\": list(range(1, 31, 2)),\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "}\n",
        "# source: https://scikit-learn.org/stable/modules/grid_search.html\n",
        "svm_estimator = SVC()\n",
        "svm_estimator_distributions = [\n",
        "    {\"C\": [1, 10, 100, 1000], \"kernel\": [\"linear\"]},\n",
        "    {\"C\": [1, 10, 100, 1000], \"gamma\": [0.001, 0.0001], \"kernel\": [\"rbf\"]},\n",
        "]"
      ],
      "metadata": {
        "id": "vWNi13tXNtBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline estimator\n",
        "train_scores, train_times, test_scores = train_test_model(\n",
        "    VECTORIZER_NAMES, knn_estimator, knn_estimator_distributions\n",
        ")"
      ],
      "metadata": {
        "id": "69fYuQlTOqZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6d8fcf32222123c2bc016e08cf8007b014ada14ae08852eb2d821271c6218457"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}