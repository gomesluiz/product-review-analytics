{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gomesluiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-12-03 09:43:48,963 - Required packages installed.\n"
     ]
    }
   ],
   "source": [
    "# Required packages.\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import string\n",
    "\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "RANDOM_SEED = 19730115\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.INFO)\n",
    "logging.info(\"Required packages installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script constants.\n",
    "DATA_ROOT_FOLDER = os.path.join(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__name__))), \"data\"\n",
    ")\n",
    "DATA_PROCESSED_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"processed\")\n",
    "DATA_EMBEDDINGS_FOLDER = os.path.join(DATA_ROOT_FOLDER, \"embeddings\")\n",
    "URL_SOURCE = \"https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv\"\n",
    "RANDOM_SEED = 19730115\n",
    "NUMBER_OF_WORDS = 50\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripts Functions.\n",
    "def load_dataset(source) -> None:\n",
    "    \"\"\"Download data from a url.\n",
    "\n",
    "    Args:\n",
    "        source (str): source data file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_csv(source)\n",
    "\n",
    "\n",
    "def load_stratify_dataset(path, stratify=False):\n",
    "    \"\"\"Get the data from csv file\n",
    "\n",
    "    Args:\n",
    "        path(str): the file complete path.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    if stratify:\n",
    "        dataset = dataset.groupby(\"polarity\", group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=0.4)\n",
    "        )\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def word_counter(text):\n",
    "    \"\"\"Word counter.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Make text lowercase, remove text in square brackets, remove punctuation and\n",
    "        remove words containing numbers.\n",
    "\n",
    "    Args:\n",
    "        text(str): string text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    text = re.sub('[``\"\"...]', \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_vector(model, dataset):\n",
    "    \n",
    "    vectorizer = model.fit_transform(dataset[\"review_text_cleaned_no_stopwords\"])\n",
    "    \n",
    "    dtm = pd.DataFrame(vectorizer.toarray(), columns=model.get_feature_names_out())\n",
    "    dtm.index = dataset.index\n",
    "    return pd.concat(\n",
    "        [dataset[[\"original_index\"]], dtm, dataset[[\"polarity\"]]],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "# def text_to_bert(text)\n",
    "def text_to_embedding(text, model, vectorizer=None, vocab=None, size=50):\n",
    "    if not vectorizer:\n",
    "        raise Exception(\"The vectorizer parameter must not be None\")\n",
    "\n",
    "    transformed = vectorizer.transform(text)\n",
    "    vectorized  = pd.DataFrame(transformed.toarray(\n",
    "    ), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    embeedings = pd.DataFrame()\n",
    "    for i in range(vectorized.shape[0]):\n",
    "        sentence = np.zeros(size)\n",
    "        for word in vocab[vectorized.iloc[i, :] > 0]:\n",
    "            if model.get_index(word, default=-1) != -1:\n",
    "                sentence = sentence + model.get_vector(word)\n",
    "            else:\n",
    "                print(\"Out of Vocabulary\")\n",
    "\n",
    "        embeedings = pd.concat([embeedings, pd.DataFrame([sentence])])\n",
    "\n",
    "    return embeedings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 09:43:57,174 - Dataset loaded from https://raw.githubusercontent.com/gomesluiz/product-review-analytics/main/data/raw/buscape.csv.\n"
     ]
    }
   ],
   "source": [
    "reviews = load_dataset(URL_SOURCE)\n",
    "logging.info(f\"Dataset loaded from {URL_SOURCE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5522/3106904682.py:5: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  reviews.loc[:, [\"review_text_cleaned_len\"]] = reviews[\"review_text_cleaned\"].apply(\n",
      "/tmp/ipykernel_5522/3106904682.py:11: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  reviews.loc[:, [\"review_text_cleaned_len_no_stopwords\"]] = reviews[\n"
     ]
    }
   ],
   "source": [
    "reviews.dropna(subset=[\"review_text\"], inplace=True)\n",
    "reviews.loc[:, [\"review_text_cleaned\"]] = reviews[\"review_text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "reviews.loc[:, [\"review_text_cleaned_len\"]] = reviews[\"review_text_cleaned\"].apply(\n",
    "    word_counter\n",
    ")\n",
    "reviews.loc[:, [\"review_text_cleaned_no_stopwords\"]] = reviews[\n",
    "    \"review_text_cleaned\"\n",
    "].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords)]))\n",
    "reviews.loc[:, [\"review_text_cleaned_len_no_stopwords\"]] = reviews[\n",
    "    \"review_text_cleaned_no_stopwords\"\n",
    "].apply(word_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_cleaned</th>\n",
       "      <th>review_text_cleaned_len</th>\n",
       "      <th>review_text_cleaned_no_stopwords</th>\n",
       "      <th>review_text_cleaned_len_no_stopwords</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_55516</td>\n",
       "      <td>Estou muito satisfeito, o visor é melhor do qu...</td>\n",
       "      <td>estou muito satisfeito o visor é melhor do que...</td>\n",
       "      <td>45</td>\n",
       "      <td>satisfeito visor melhor imaginava boas imagens...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>minus_1_105339</td>\n",
       "      <td>\"muito boa\\n\\nO que gostei: preco\\n\\nO que não...</td>\n",
       "      <td>muito boa  o que gostei preco  o que não goste...</td>\n",
       "      <td>12</td>\n",
       "      <td>boa gostei preco gostei poderia</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23_382139</td>\n",
       "      <td>Rápida, ótima qualidade de impressão e fácil d...</td>\n",
       "      <td>rápida ótima qualidade de impressão e fácil de...</td>\n",
       "      <td>37</td>\n",
       "      <td>rápida ótima qualidade impressão fácil usar pr...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_446456</td>\n",
       "      <td>Produto de ótima qualidade em todos os quesito!</td>\n",
       "      <td>produto de ótima qualidade em todos os quesito</td>\n",
       "      <td>8</td>\n",
       "      <td>produto ótima qualidade todos quesito</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_11324</td>\n",
       "      <td>Precisava comprar uma tv compatível com meu dv...</td>\n",
       "      <td>precisava comprar uma tv compatível com meu dv...</td>\n",
       "      <td>38</td>\n",
       "      <td>precisava comprar tv compatível dvd esra melho...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                                        review_text  \\\n",
       "0         4_55516  Estou muito satisfeito, o visor é melhor do qu...   \n",
       "1  minus_1_105339  \"muito boa\\n\\nO que gostei: preco\\n\\nO que não...   \n",
       "2       23_382139  Rápida, ótima qualidade de impressão e fácil d...   \n",
       "3        2_446456    Produto de ótima qualidade em todos os quesito!   \n",
       "4         0_11324  Precisava comprar uma tv compatível com meu dv...   \n",
       "\n",
       "                                 review_text_cleaned  review_text_cleaned_len  \\\n",
       "0  estou muito satisfeito o visor é melhor do que...                       45   \n",
       "1  muito boa  o que gostei preco  o que não goste...                       12   \n",
       "2  rápida ótima qualidade de impressão e fácil de...                       37   \n",
       "3     produto de ótima qualidade em todos os quesito                        8   \n",
       "4  precisava comprar uma tv compatível com meu dv...                       38   \n",
       "\n",
       "                    review_text_cleaned_no_stopwords  \\\n",
       "0  satisfeito visor melhor imaginava boas imagens...   \n",
       "1                    boa gostei preco gostei poderia   \n",
       "2  rápida ótima qualidade impressão fácil usar pr...   \n",
       "3              produto ótima qualidade todos quesito   \n",
       "4  precisava comprar tv compatível dvd esra melho...   \n",
       "\n",
       "   review_text_cleaned_len_no_stopwords  polarity  \n",
       "0                                    25         1  \n",
       "1                                     5         1  \n",
       "2                                    22         1  \n",
       "3                                     5         1  \n",
       "4                                    17         1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the original polarity to -1 from 0, nan to 0.\n",
    "reviews_cleaned = reviews[\n",
    "    [\n",
    "        \"original_index\",\n",
    "        \"review_text\",\n",
    "        \"review_text_cleaned\",\n",
    "        \"review_text_cleaned_len\",\n",
    "        \"review_text_cleaned_no_stopwords\",\n",
    "        \"review_text_cleaned_len_no_stopwords\",\n",
    "        \"polarity\",\n",
    "    ]\n",
    "].copy()\n",
    "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].replace({0: -1, np.nan: 0})\n",
    "reviews_cleaned[\"polarity\"] = reviews_cleaned[\"polarity\"].astype(int)\n",
    "#\n",
    "reviews_cleaned.dropna(subset=[\"review_text_cleaned_no_stopwords\"], inplace=True)\n",
    "reviews_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_dataset, reviews_test_dataset = train_test_split(\n",
    "    reviews_cleaned,\n",
    "    stratify=reviews_cleaned[\"polarity\"],\n",
    "    test_size=0.20,\n",
    "    random_state=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 11:24:31,422 - The cv vectorizer train matrix has 67992 rows and 52 columns\n",
      "2022-12-03 11:24:31,493 - The cv vectorizer test matrix has 16998 rows and 52 columns\n",
      "2022-12-03 11:24:35,984 - The tv vectorizer train matrix has 67992 rows and 52 columns\n",
      "2022-12-03 11:24:35,986 - The tv vectorizer test matrix has 16998 rows and 52 columns\n"
     ]
    }
   ],
   "source": [
    "# Counter vectorizer\n",
    "vectorizers = {\n",
    "    \"cv\": CountVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
    "    \"tv\": TfidfVectorizer(stop_words=stopwords, max_features=NUMBER_OF_WORDS),\n",
    "}\n",
    "\n",
    "if not os.path.exists(DATA_PROCESSED_FOLDER):\n",
    "    os.makedirs(DATA_PROCESSED_FOLDER)\n",
    "\n",
    "for name, model in vectorizers.items():\n",
    "    reviews_train_vectorized = text_to_vector(model, reviews_train_dataset)\n",
    "    reviews_train_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_train_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
    "\n",
    "    reviews_test_vectorized = text_to_vector(model, reviews_test_dataset)\n",
    "    reviews_test_vectorized.to_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
    "    \n",
    "    logging.info(\n",
    "        f\"The {name} vectorizer train matrix has {reviews_train_vectorized.shape[0]} rows and {reviews_train_vectorized.shape[1]} columns\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"The {name} vectorizer test matrix has {reviews_test_vectorized.shape[0]} rows and {reviews_test_vectorized.shape[1]} columns\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 09:44:16,802 - Downloading: word2vec_cbow_s50\n",
      "2022-12-03 09:44:36,091 - Downloading: word2vec_skip_s50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m                 in_file_zip\u001b[39m.\u001b[39mextractall(out_folder_path)\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m model, architecture \u001b[39min\u001b[39;00m EMBEDDING_NAMES:\n\u001b[0;32m---> 31\u001b[0m     download_extract(model, architecture)\n",
      "Cell \u001b[0;32mIn[57], line 26\u001b[0m, in \u001b[0;36mdownload_extract\u001b[0;34m(model, architecture)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(out_file_path):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m urlopen(url) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mwith\u001b[39;00m ZipFile(BytesIO(response\u001b[39m.\u001b[39;49mread())) \u001b[39mas\u001b[39;00m in_file_zip:\n\u001b[1;32m     27\u001b[0m             in_file_zip\u001b[39m.\u001b[39mextractall(out_folder_path)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:481\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    482\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    624\u001b[0m     \u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[1;32m    626\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "EMBEDDING_NAMES = [\n",
    "    [\"word2vec\", \"cbow_s50\"],\n",
    "    [\"word2vec\", \"skip_s50\"],\n",
    "    [\"fasttext\", \"cbow_s50\"],\n",
    "    [\"fasttext\", \"skip_s50\"],\n",
    "    [\"glove\", \"glove_s50\"],\n",
    "    [\"wang2vec\", \"cbow_s50\"],\n",
    "    [\"wang2vec\", \"skip_s50\"],\n",
    "]\n",
    "\n",
    "\n",
    "def download_extract(model, architecture):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    url = f\"http://143.107.183.175:22980/download.php?file=embeddings/{model}/{architecture}.zip\"\n",
    "    out_folder_path = os.path.join(DATA_EMBEDDINGS_FOLDER, model)\n",
    "    out_file_path = os.path.join(out_folder_path, architecture)\n",
    "    logging.info(f\"Downloading: {model}_{architecture}\")\n",
    "    if not os.path.exists(out_file_path):\n",
    "        with urlopen(url) as response:\n",
    "            with ZipFile(BytesIO(response.read())) as in_file_zip:\n",
    "                in_file_zip.extractall(out_folder_path)\n",
    "\n",
    "\n",
    "for model, architecture in EMBEDDING_NAMES:\n",
    "    download_extract(model, architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned fast text embedding.\n",
    "logging.info(\"Load fast text embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"fasttext\")\n",
    "fasttext_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "fasttext_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned glove embedding.\n",
    "logging.info(\"Load glove embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"glove\")\n",
    "glove_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER,\"glove_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned wang2vec embedding.\n",
    "logging.info(\"Load wang2vec embeddings.\")\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"wang2vec\")\n",
    "wang2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "wang2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trainned word2vec embedding.\n",
    "DATA_EMBEDDING_FOLDER=os.path.join(DATA_EMBEDDINGS_FOLDER, \"word2vec\")\n",
    "word2vec_cbow_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"cbow_s50.txt\"))\n",
    "word2vec_skip_s50 = KeyedVectors.load_word2vec_format(os.path.join(DATA_EMBEDDING_FOLDER, \"skip_s50.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fasttext_cbow_s50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_models \u001b[39m=\u001b[39m [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n\u001b[1;32m      2\u001b[0m                     wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n\u001b[1;32m      4\u001b[0m reviews_vectorized \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATA_PROCESSED_FOLDER, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuscape_reviews_test_dataset_\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_s\u001b[39m\u001b[39m{\u001b[39;00mNUMBER_OF_WORDS\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m name, model \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(EMBEDDING_NAMES, embedding_models):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fasttext_cbow_s50' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_models = [fasttext_cbow_s50, fasttext_skip_s50, glove_s50,\n",
    "                    wang2vec_cbow_s50, wang2vec_skip_s50, word2vec_cbow_s50, word2vec_skip_s50]\n",
    "\n",
    "reviews_vectorized = pd.read_pickle(os.path.join(DATA_PROCESSED_FOLDER, f\"buscape_reviews_test_dataset_{name}_s{NUMBER_OF_WORDS}.pkl\"))\n",
    "\n",
    "for name, model in zip(EMBEDDING_NAMES, embedding_models):\n",
    "    reviews_train_dtm = text_to_embedding(\n",
    "        reviews_train_dataset['review_text'], model, vectorizers[\"tv\"], reviews_vectorized.columns[1:-1], 50)\n",
    "    reviews_train_processed = pd.concat([reviews_train_dataset.reset_index()[['original_index']], reviews_train_dtm.reset_index(\n",
    "        drop=True), reviews_train_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_train_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_train_dataset_{name[0]}_{name[1]}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorized train dataframe has {reviews_train_processed.shape[0]} rows and {reviews_train_processed.shape[1]} columns\")\n",
    "\n",
    "    reviews_test_dtm = text_to_embedding(\n",
    "        reviews_test_dataset['review_text'], model, vectorizers[\"tv\"], reviews_vectorized.columns[1:-1], 50)\n",
    "    reviews_test_processed = pd.concat([reviews_test_dataset.reset_index()[['original_index']], reviews_test_dtm.reset_index(\n",
    "        drop=True), reviews_test_dataset.reset_index()[['polarity']]], axis=1, ignore_index=True)\n",
    "    reviews_test_processed.to_pickle(\n",
    "        f\"../data/processed/buscape_reviews_test_dataset_{name[0]}_{name[1]}.pkl\")\n",
    "    print(\n",
    "        f\"The {name} vectorized test dataframe has {reviews_test_processed.shape[0]} rows and {reviews_test_processed.shape[1]} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d8fcf32222123c2bc016e08cf8007b014ada14ae08852eb2d821271c6218457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
